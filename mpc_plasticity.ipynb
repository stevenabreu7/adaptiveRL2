{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mpc_utils import load_train_test_data_27s, load_train_test_data\n",
    "import torch\n",
    "from models.backpropamine_torch import BackpropamineRNN\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, log_folder, update_freq=10) -> None:\n",
    "        self.log_folder = log_folder\n",
    "        os.makedirs(log_folder, exist_ok=True)\n",
    "        self.datastore = {}\n",
    "        self.counter = {}\n",
    "        self.update_freq = update_freq\n",
    "\n",
    "    def write_scalar(self, scalar: float, filename: str, update_freq=None):\n",
    "        self.datastore[filename] = self.datastore.get(filename, []) + [scalar]\n",
    "        # update every self.update_freq steps\n",
    "        self.counter[filename] = self.counter.get(filename, 0) + 1\n",
    "        ufreq = update_freq if update_freq is not None else self.update_freq\n",
    "        if self.counter[filename] >= ufreq:\n",
    "            self.save()\n",
    "            self.counter[filename] = 0\n",
    "\n",
    "    def save(self):\n",
    "        for filename, data in self.datastore.items():\n",
    "            np.savetxt(f'./{self.log_folder}/{filename}.txt', np.array(data))\n",
    "            # np.save(f'./{self.log_folder}/{filename}.npy', np.array(data))\n",
    "\n",
    "    def close(self):\n",
    "        self.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((192, 1350, 7), (192, 1350, 1), (33, 1350, 7), (33, 1350, 1), (192, 1350, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_train_test_data_27s(include_L=True)\n",
    "l_train, l_test = x_train[:, :, 7:], x_test[:, :, 7:]\n",
    "x_train, x_test = x_train[:, :, :7], x_test[:, :, :7]\n",
    "\n",
    "x = np.concatenate([x_train, x_test], axis=0)\n",
    "y = np.concatenate([y_train, y_test], axis=0)\n",
    "l = np.concatenate([l_train, l_test], axis=0)\n",
    "low_thr = 0.12\n",
    "up_thr = 0.415\n",
    "tr_idx = (l[:,0,0] > low_thr) & (l[:,0,0] < up_thr)\n",
    "te_idx = ~tr_idx\n",
    "x_train, y_train, l_train = x[tr_idx], y[tr_idx], l[tr_idx]\n",
    "x_test, y_test, l_test = x[te_idx], y[te_idx], l[te_idx]\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape, l_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lmu_torch import LMUCell\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "class BackpropamineLMU(torch.nn.Module):\n",
    "    def __init__(self, input_size, lmu_hidden_size, lmu_memory_size, \n",
    "                 lmu_theta, rnn_hidden_size, output_size, \n",
    "                 plasticity_noise=None, device='cpu'):\n",
    "        super(BackpropamineLMU, self).__init__()\n",
    "        self.lmu_cell = LMUCell(\n",
    "            input_size=input_size, hidden_size=lmu_hidden_size, \n",
    "            memory_size=lmu_memory_size, theta=lmu_theta\n",
    "        )\n",
    "        self.bp_rnn = BackpropamineRNN(\n",
    "            isize=lmu_hidden_size, hsize=rnn_hidden_size, \n",
    "            osize=output_size, random_plasticity=plasticity_noise is not None,\n",
    "            plasticity_noise=plasticity_noise, device=DEVICE\n",
    "        )\n",
    "        self.device = device\n",
    "        self.lmu_cell.to(device)\n",
    "        self.bp_rnn.to(device)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # assume x is [batch_size, seq_len, input_size]\n",
    "        # hidden = (lmu_state, bp_hid)\n",
    "        T = x.shape[1]\n",
    "        if hidden is None:\n",
    "            lmu_hid = torch.zeros(x.shape[0], self.lmu_cell.hidden_size, device=self.device)\n",
    "            lmu_mem = torch.zeros(x.shape[0], self.lmu_cell.memory_size, device=self.device)\n",
    "            lmu_state = (lmu_hid, lmu_mem)\n",
    "            bp_hid = self.bp_rnn.initialZeroStateHebb(x.shape[0])\n",
    "        else:\n",
    "            lmu_state, bp_hid = hidden\n",
    "        outputs = []\n",
    "        for t in range(T):\n",
    "            # print('x', x.shape)\n",
    "            # print('lmu_state', lmu_state.shape)\n",
    "            lmu_state = self.lmu_cell(x[:,t,:], lmu_state)\n",
    "            out_t, bp_hid = self.bp_rnn(lmu_state[0], bp_hid) # pass lmu_state[0] = lmu_hid\n",
    "            outputs.append(out_t)\n",
    "        return torch.stack(outputs, dim=1), (lmu_state, bp_hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, logger, x_train, y_train, n_batches, batch_size, optimizer, loss, device):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        hidden = None\n",
    "\n",
    "        epoch_loss = []\n",
    "        model.train()\n",
    "        for batch_idx in range(n_batches-1):\n",
    "            if hidden is not None:\n",
    "                # reset hebbian hidden state\n",
    "                hidden = (hidden[0], model.bp_rnn.initialZeroStateHebb(batch_size))\n",
    "\n",
    "            x = x_train[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "            y = y_train[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "            x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "            y = torch.tensor(y, dtype=torch.float32, device=device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, hidden = model(x, None)\n",
    "\n",
    "            l = loss(y_pred, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss.append(l.item())\n",
    "\n",
    "        avg_epoch_loss = np.array(epoch_loss).mean()\n",
    "        logger.write_scalar(avg_epoch_loss, 'train_loss')\n",
    "        logger.write_scalar(time.time()-epoch_start, 'ep_dur')\n",
    "\n",
    "        if epoch % 10 == 9 or epoch in [0,1,2]:\n",
    "            print(epoch+1, avg_epoch_loss, time.time()-epoch_start)\n",
    "        else:\n",
    "            print(epoch+1, avg_epoch_loss, time.time()-epoch_start, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_folder = f'logs/mpc_plasticity/rp05x10-01x20_{time.strftime(\"%m%d_%H%M\", time.gmtime())}'\n",
    "logger = Logger(log_folder, update_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "batch_size = 32\n",
    "n_batches = x_train.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.4832513093948364 32.22351408004761\n",
      "2 0.32660278081893923 32.0911009311676\n",
      "3 0.33275737166404723 31.59447979927063\n",
      "10 0.30536890029907227 31.65047287940979\n"
     ]
    }
   ],
   "source": [
    "model = BackpropamineLMU(input_size=7, lmu_hidden_size=128, lmu_memory_size=128,\n",
    "                         lmu_theta=100, rnn_hidden_size=128, output_size=1, plasticity_noise=0.05,\n",
    "                         device=DEVICE)\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs = 10\n",
    "train(epochs, model, logger, x_train, y_train, n_batches, batch_size, optimizer, loss, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.29240294694900515 31.141382932662964\n",
      "2 0.2666308730840683 31.36859703063965\n",
      "3 0.2577412873506546 31.405122756958008\n",
      "10 0.053297373652458194 109.45631194114685\n",
      "20 0.02183917872607708 28.869108438491825\n"
     ]
    }
   ],
   "source": [
    "model.bp_rnn.plasticity_noise = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs = 20\n",
    "train(epochs, model, logger, x_train, y_train, n_batches, batch_size, optimizer, loss, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{log_folder}/model_pre.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: freeze all weights except plasticity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters())\n\u001b[1;32m     10\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m---> 11\u001b[0m train(epochs, model, logger, x_train, y_train, n_batches, batch_size, optimizer, loss, DEVICE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "model = BackpropamineLMU(input_size=7, lmu_hidden_size=128, lmu_memory_size=128,\n",
    "                         lmu_theta=100, rnn_hidden_size=128, output_size=1, plasticity_noise=None,\n",
    "                         device=DEVICE)\n",
    "model.to(DEVICE)\n",
    "# load model with random plasticity - insert new value for plasticity matrix\n",
    "x = torch.load(f'{log_folder}/model.pt')\n",
    "x['bp_rnn.alpha'] = torch.rand(128, 128) * 0.001\n",
    "model.load_state_dict(x)\n",
    "# train final model\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs = 100\n",
    "train(epochs, model, logger, x_train, y_train, n_batches, batch_size, optimizer, loss, DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logs\n",
    "\n",
    "- random_plasiticy 0.01 works fine (losses per epoch: 0.22, 0.10, 0.06, 0.03)\n",
    "- random_plasticity 0.05 works as well (losses per epoch: 0.37, 0.33, 0.32, 0.30)\n",
    "- random_plasticity 0.05 for 10 epochs, then 0.01 for 10 epochs -> final loss "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpclmu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
